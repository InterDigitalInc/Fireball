{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a MobileNetV2 model to ONNX\n",
    "You can export any Fireball model to ONNX using the [exportToOnnx](https://interdigitalinc.github.io/Fireball/html/source/model.html#fireball.model.Model.exportToOnnx) function. This notebook shows how to use this function to create an ONNX model. It assumes that a trained MobileNetV2 model already exists in the ```Models``` directory. Please refer to the notebook [Image Classification with MobileNetV2](MobileNetV2.ipynb) for more info about using a pretrained MobileNetV2 model.\n",
    "\n",
    "Fireball can also export models with reduced number of parameters, pruned models, and quatized models. Please refer to the following notebooks for more information:\n",
    "\n",
    "- [Reducing number of parameters of MobileNetV2 Model](MobileNetV2-Reduce.ipynb)\n",
    "- [Pruning MobileNetV2 Model](MobileNetV2-Prune.ipynb)\n",
    "- [Quantizing MobileNetV2 Model](MobileNetV2-Quantize.ipynb)\n",
    "\n",
    "Note: Fireball uses the [onnx](https://github.com/onnx/onnx) python package to export models to ONNX. We also use the [onnxruntime](https://onnxruntime.ai) here to run and evaluate the onnx models.\n",
    "\n",
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/MobileNetV2RRPRQR.fbm\" ... Done.\n",
      "Creating the fireball model \"MobileNetV2\" ... Done.\n",
      "\n",
      "Scope            InShape       Comments                 OutShape      Activ.   Post Act.        # of Params\n",
      "---------------  ------------  -----------------------  ------------  -------  ---------------  -----------\n",
      "IN_IMG                         Image Size: 224x224x3    224 224 3     None                      0          \n",
      "S1_L1_CONV       224 224 3     KSP: 3 2 0x1x0x1         112 112 32    None                      435        \n",
      "S1_L2_BN         112 112 32                             112 112 32    ReLU     x<6.0            128        \n",
      "S2_L1_DWCN       112 112 32    KSP: 3 1 s               112 112 32    None                      279        \n",
      "S2_L2_BN         112 112 32                             112 112 32    ReLU     x<6.0            128        \n",
      "S2_L3_CONV       112 112 32    KSP: 1 1 s               112 112 16    None                      265        \n",
      "S2_L4_BN         112 112 16                             112 112 16    None                      64         \n",
      "S3_L1_MN2        112 112 16    6 layers                 56 56 24      None                      4,620      \n",
      "S3_L2_MN1S       56 56 24      2 Paths, 7 layers        56 56 24      None                      7,531      \n",
      "S4_L1_MN2        56 56 24      6 layers                 28 28 32      None                      8,686      \n",
      "S4_L2_MN1S       28 28 32      2 Paths, 7 layers        28 28 32      None                      11,861     \n",
      "S4_L3_MN1S       28 28 32      2 Paths, 7 layers        28 28 32      None                      11,721     \n",
      "S5_L1_MN2        28 28 32      6 layers                 14 14 64      None                      17,171     \n",
      "S5_L2_MN1S       14 14 64      2 Paths, 7 layers        14 14 64      None                      36,870     \n",
      "S5_L3_MN1S       14 14 64      2 Paths, 7 layers        14 14 64      None                      36,570     \n",
      "S5_L4_MN1S       14 14 64      2 Paths, 7 layers        14 14 64      None                      36,738     \n",
      "S6_L1_MN1        14 14 64      6 layers                 14 14 96      None                      48,709     \n",
      "S6_L2_MN1S       14 14 96      2 Paths, 7 layers        14 14 96      None                      79,110     \n",
      "S6_L3_MN1S       14 14 96      2 Paths, 7 layers        14 14 96      None                      79,553     \n",
      "S7_L1_MN2        14 14 96      6 layers                 7 7 160       None                      111,288    \n",
      "S7_L2_MN1S       7 7 160       2 Paths, 7 layers        7 7 160       None                      200,819    \n",
      "S7_L3_MN1S       7 7 160       2 Paths, 7 layers        7 7 160       None                      202,225    \n",
      "S7_L4_MN1        7 7 160       6 layers                 7 7 320       None                      294,350    \n",
      "S8_L1_CONV       7 7 320       KSP: 1 1 s, LR184        7 7 1280      None                      164,260    \n",
      "S8_L2_BN         7 7 1280                               1 1 1280      ReLU     CLP->GAP         5,120      \n",
      "S8_L3_FC         1 1 1280      LR304                    1000          None                      447,504    \n",
      "OUT_CLASS        1000          1000 classes             1000          None                      0          \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "                                                                  Total Number of parameters: 1,806,005  \n",
      "  Processed 50000 Sample. (Time: 35.80 Sec.)                              \n",
      "\n",
      "Observed Accuracy:  0.65734\n"
     ]
    }
   ],
   "source": [
    "from fireball import Model\n",
    "from fireball.datasets.imagenet import ImageNetDSet\n",
    "gpus='upto4'\n",
    "\n",
    "# Create the test dataset for evaluation.\n",
    "testDs = ImageNetDSet.makeDatasets('Test', batchSize=256, preProcessing='Crop256Tf', numWorkers=8)\n",
    "\n",
    "orgFileName = \"Models/MobileNetV2RRPRQR.fbm\"  # Reduced - Retrained - Pruned - Retrained - Quantized - Retrained\n",
    "\n",
    "model = Model.makeFromFile(orgFileName, testDs=testDs, gpus=gpus)\n",
    "model.printLayersInfo()\n",
    "model.initSession()\n",
    "results = model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model and check the exported ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting to ONNX model \"Models/MobileNetV2RRPRQR.onnx\" ... \n",
      "    Processed all 27 layers.                                     \n",
      "    Saving to \"Models/MobileNetV2RRPRQR.onnx\" ... Done.\n",
      "Done (57.92 Sec.)\n"
     ]
    }
   ],
   "source": [
    "from fireball.datasets.imagenet import ImageNetDSet\n",
    "\n",
    "onnxFileName = orgFileName.replace(\".fbm\",\".onnx\")\n",
    "\n",
    "model.exportToOnnx(onnxFileName, runQuantized=True, classNames=ImageNetDSet.classNames, \n",
    "                   modelDocStr=\"Fireball example: MobileNetV2 Model\")\n",
    "\n",
    "# Check the exported model. This throws exceptions if something is wrong with the exported model.\n",
    "import onnx\n",
    "from onnx import shape_inference\n",
    "\n",
    "onnxModel = onnx.load(onnxFileName)\n",
    "onnx.checker.check_model(onnxModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using netron to visualize the exported model\n",
    "We can now visualize the model's network structure using the [netron](https://github.com/lutzroeder/netron) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'Models/MobileNetV2RRPRQR.onnx' at http://10.1.16.58:8084\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':      # Running on MAC\n",
    "    netron.start(onnxFileName)   \n",
    "else:\n",
    "    import socket\n",
    "    hostIp = socket.gethostbyname(socket.gethostname())\n",
    "    netron.start(onnxFileName, address=(hostIp,8084))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the exported model\n",
    "To verify the exported model, we can now run inference on it. Here we load an image and do the required pre-processing before passing it to the exported model as input. We then print the top-3 most probable predicted labels for the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: InputImage, ouput: ClassProbs\n",
      "Top-3 Classes (For \"CoffeeMug.jpg\"):\n",
      "    coffee_mug (0.611419)\n",
      "    cup (0.335758)\n",
      "    pitcher (0.014754)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "imageFileName = 'CoffeeMug.jpg'\n",
    "\n",
    "img = cv2.imread(imageFileName)     # Reads image in BGR order\n",
    "img = np.float32(img)[..., ::-1]    # Convert to RGB\n",
    "\n",
    "# Resize the image to 256x256\n",
    "imgSize = img.shape[:2]\n",
    "ratio = 256.0/min(imgSize)\n",
    "newSize = (int(np.round(imgSize[1]*ratio)), int(np.round(imgSize[0]*ratio)))\n",
    "\n",
    "# Note: INTER_AREA is best when shrinking and CV_INTER_CUBIC is best when enlarging\n",
    "img = cv2.resize(img, newSize,  interpolation = (cv2.INTER_AREA if ratio<1.0 else cv2.INTER_CUBIC))\n",
    "\n",
    "# Now crop the center 224x224 image\n",
    "dw = newSize[0] - 224\n",
    "dh = newSize[1] - 224\n",
    "resizedImg = img[dh//2:dh//2+224, dw//2:dw//2+224,:]\n",
    "\n",
    "# Normalize the image values to the range: -1 .. 1\n",
    "inputImage = ((np.float32(resizedImg)/127.5)-1.0)\n",
    "inputImage = np.transpose(inputImage, (2,0,1))    # Onnx expects channel-first images\n",
    "\n",
    "# Inference using the ONNX model and \"onnxruntime\"\n",
    "import onnxruntime as ort\n",
    "options = ort.SessionOptions()\n",
    "options.intra_op_num_threads = 4\n",
    "session = ort.InferenceSession(onnxModel.SerializeToString(), options, providers=['CPUExecutionProvider'])\n",
    "    \n",
    "print('input: %s, ouput: %s'%(session.get_inputs()[0].name,session.get_outputs()[0].name))\n",
    "y = session.run(['ClassProbs'],{'InputImage':[inputImage]})\n",
    "\n",
    "classProbs = y[0][0]\n",
    "top3Indexes = np.argsort(classProbs)[-3:][::-1]    # Indexes of classes with 3 highest probs (decreasing order)\n",
    "top3Porbs = classProbs[top3Indexes]\n",
    "print('Top-3 Classes (For \"%s\"):'%(imageFileName))\n",
    "for i in range(3):\n",
    "    print('    %s (%f)'%(ImageNetDSet.classNames[top3Indexes[i]], top3Porbs[i])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also look at\n",
    "\n",
    "[Exporting MobileNetV2 Model to CoreML](MobileNetV2-CoreML.ipynb)\n",
    "\n",
    "[Exporting MobileNetV2 Model to TensorFlow](MobileNetV2-TF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n",
    "[Image Classification with MobileNetV2](MobileNetV2.ipynb)\n",
    "\n",
    "[Reducing number of parameters of MobileNetV2 Model](MobileNetV2-Reduce.ipynb)\n",
    "\n",
    "[Pruning MobileNetV2 Model](MobileNetV2-Prune.ipynb)\n",
    "\n",
    "[Quantizing MobileNetV2 Model](MobileNetV2-Quantize.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
