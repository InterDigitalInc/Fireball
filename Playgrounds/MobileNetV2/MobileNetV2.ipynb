{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with MobileNetV2\n",
    "MobileNetV2 (https://arxiv.org/abs/1801.04381) builds upon the ideas from MobileNetV1, using depthwise separable convolution as efficient building blocks. However, V2 introduces two new features to the architecture:\n",
    "1. linear bottlenecks between the layers, and\n",
    "2. shortcut connections between the bottlenecks.\n",
    "\n",
    "Overall, the MobileNetV2 is faster for the same accuracy across the entire latency spectrum. It uses 2x fewer operations, needs 30% fewer parameters and is about 30-40% faster than MobileNetV1, all while achieving higher accuracy.\n",
    "\n",
    "In this playground we load a pre-trained MobileNetV2 model and do some inference and evaluation on ImageNet dataset.\n",
    "\n",
    "Note: Fireball uses the OpenCV python package to process images in the ImageNet dataset. If this package is not installed already, you can just run the following command in a new cell and restart the kernel.\n",
    "\n",
    "```\n",
    "%pip install opencv-python\n",
    "```\n",
    "\n",
    "## Create an ImageNet dataset\n",
    "Lets first load the ImageNet dataset and see dataset statistics.\n",
    "\n",
    "**NOTE**: MobileNetV2 uses the \"Crop256Tf\" pre-processing for the images. This pre-processing first resizes the image (keeping the aspect ratio) so that its smaller dimension is 256. It then crops a 224x224 image from the center of the resized image. The image is in RGB format and the values are normalized to values between -1 and 1. Please refer to Fireball's ```imagenet.py``` file for more information about different types of pre-processing supported by Fireball.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing ImageNet dataset ... Done.\n",
      "ImageNetDSet Dataset Info:\n",
      "    Number of Classes .............................. 1000\n",
      "    Dataset Location ............................... /data/ImageNet/\n",
      "    Number of Training Samples ..................... 1281167\n",
      "    Number of Test Samples ......................... 50000\n",
      "    Sample Shape ................................... (224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time, os\n",
    "from fireball import Model, Block, myPrint\n",
    "from fireball.datasets.imagenet import ImageNetDSet\n",
    "\n",
    "gpus=\"0,1,2,3\"\n",
    "\n",
    "myPrint('\\nPreparing ImageNet dataset ... ', False)\n",
    "trainDs,testDs = ImageNetDSet.makeDatasets('Train,Test', batchSize=256, preProcessing='Crop256Tf', numWorkers=8)\n",
    "myPrint('Done.')\n",
    "ImageNetDSet.printDsInfo(trainDs, testDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a MobileNetV2 Fireball model and print the model information\n",
    "Let's load the model information from a pre-trained fireball model and print information about different layers of the model. For your information, the MobileNetV2's layer info text and blocks are as follows. Since we already have a trained fbm file for MobileNetV2, we don't need them here.\n",
    "\n",
    "```\n",
    "blocks = [ \n",
    "    Block('MN1|x_expansion_i,o_outDept_i|' +     # MobileNet Block with Stride 1 No shortcut\n",
    "          'add|' +\n",
    "          'CONV_K1_O%x_Ps_B0,BN:ReLU:CLP_H6,DWCN_K3_S1_Ps_B0,BN:ReLU:CLP_H6,CONV_K1_O%o_Ps_B0,BN'),\n",
    "\n",
    "    Block('MN1S|x_expansion_i,o_outDept_i|' +    # MobileNet Block with Stride 1 With shortcut\n",
    "          'add|' +\n",
    "          'CONV_K1_O%x_Ps_B0,BN:ReLU:CLP_H6,DWCN_K3_Ps_B0,BN:ReLU:CLP_H6,CONV_K1_O%o_Ps_B0,BN;ID'),\n",
    "\n",
    "    Block('MN2|x_expansion_i,o_outDept_i|' +     # MobileNet Block with Stride 2 No shortcut\n",
    "          'add|' +\n",
    "          'CONV_K1_O%x_Ps_B0,BN:ReLU:CLP_H6,DWCN_K3_S2_P0x1x0x1_B0,BN:ReLU:CLP_H6,CONV_K1_O%o_Ps_B0,BN')\n",
    "    ]\n",
    "\n",
    "layers = ('IMG_S224_D3;CONV_K3_O32_S2_P0x1x0x1_B0,BN:ReLU:CLP_H6;'  +         # Input Stages\n",
    "          'DWCN_K3_S1_Ps_B0,BN:ReLU:CLP_H6,CONV_K1_O16_S1_Ps_B0,BN;' +        # Block 0\n",
    "          'MN2_X96_O24,MN1S_X144_O24;' +                                      # Blocks 1, 2\n",
    "          'MN2_X144_O32,MN1S_X192_O32,MN1S_X192_O32;' +                       # Blocks 3, 4, 5\n",
    "          'MN2_X192_O64,MN1S_X384_O64,MN1S_X384_O64,MN1S_X384_O64;' +         # Blocks 6, 7, 8, 9\n",
    "          'MN1_X384_O96,MN1S_X576_O96,MN1S_X576_O96;' +                       # Blocks 10, 11, 12\n",
    "          'MN2_X576_O160,MN1S_X960_O160,MN1S_X960_O160,MN1_X960_O320;' +      # Blocks 13, 14, 15, 16\n",
    "          'CONV_K1_O1280_Ps_B0,BN:ReLU:CLP_H6:GAP,FC_O1000:None;CLASS_C1000') # Output Stages\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/MobileNetV2.fbm\" ... Done.\n",
      "Creating the fireball model \"MobileNetV2\" ... Done.\n",
      "\n",
      "Scope            InShape       Comments                 OutShape      Activ.   Post Act.        # of Params\n",
      "---------------  ------------  -----------------------  ------------  -------  ---------------  -----------\n",
      "IN_IMG                         Image Size: 224x224x3    224 224 3     None                      0          \n",
      "S1_L1_CONV       224 224 3     KSP: 3 2 0x1x0x1         112 112 32    None                      864        \n",
      "S1_L2_BN         112 112 32                             112 112 32    ReLU     x<6.0            128        \n",
      "S2_L1_DWCN       112 112 32    KSP: 3 1 s               112 112 32    None                      288        \n",
      "S2_L2_BN         112 112 32                             112 112 32    ReLU     x<6.0            128        \n",
      "S2_L3_CONV       112 112 32    KSP: 1 1 s               112 112 16    None                      512        \n",
      "S2_L4_BN         112 112 16                             112 112 16    None                      64         \n",
      "S3_L1_MN2        112 112 16    6 layers                 56 56 24      None                      5,568      \n",
      "S3_L2_MN1S       56 56 24      2 Paths, 6 layers        56 56 24      None                      9,456      \n",
      "S4_L1_MN2        56 56 24      6 layers                 28 28 32      None                      10,640     \n",
      "S4_L2_MN1S       28 28 32      2 Paths, 6 layers        28 28 32      None                      15,680     \n",
      "S4_L3_MN1S       28 28 32      2 Paths, 6 layers        28 28 32      None                      15,680     \n",
      "S5_L1_MN2        28 28 32      6 layers                 14 14 64      None                      21,952     \n",
      "S5_L2_MN1S       14 14 64      2 Paths, 6 layers        14 14 64      None                      55,936     \n",
      "S5_L3_MN1S       14 14 64      2 Paths, 6 layers        14 14 64      None                      55,936     \n",
      "S5_L4_MN1S       14 14 64      2 Paths, 6 layers        14 14 64      None                      55,936     \n",
      "S6_L1_MN1        14 14 64      6 layers                 14 14 96      None                      68,352     \n",
      "S6_L2_MN1S       14 14 96      2 Paths, 6 layers        14 14 96      None                      120,768    \n",
      "S6_L3_MN1S       14 14 96      2 Paths, 6 layers        14 14 96      None                      120,768    \n",
      "S7_L1_MN2        14 14 96      6 layers                 7 7 160       None                      157,888    \n",
      "S7_L2_MN1S       7 7 160       2 Paths, 6 layers        7 7 160       None                      324,160    \n",
      "S7_L3_MN1S       7 7 160       2 Paths, 6 layers        7 7 160       None                      324,160    \n",
      "S7_L4_MN1        7 7 160       6 layers                 7 7 320       None                      478,400    \n",
      "S8_L1_CONV       7 7 320       KSP: 1 1 s               7 7 1280      None                      409,600    \n",
      "S8_L2_BN         7 7 1280                               1 1 1280      ReLU     CLP->GAP         5,120      \n",
      "S8_L3_FC         1 1 1280                               1000          None                      1,281,000  \n",
      "OUT_CLASS        1000          1000 classes             1000          None                      0          \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "                                                                  Total Number of parameters: 3,538,984  \n"
     ]
    }
   ],
   "source": [
    "model = Model.makeFromFile(\"Models/MobileNetV2.fbm\", testDs=testDs, gpus=gpus)\n",
    "model.printLayersInfo()\n",
    "model.initSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick inference demo\n",
    "Now let's show how this model can be used to classify an image. Here we are using a JPEG image of a coffee mug.\n",
    "The function ```getPreprocessedImage```, loads, scales, and preprocesses the image before returning it as numpy array of floating point numbers. We can pass the preprocessed image to the ```inferOne``` function to get the probabilities for each one of 1000 classes. We then print the top-3 classes with highest probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Classes (For \"CoffeeMug.jpg\"):\n",
      "    coffee_mug (0.777315)\n",
      "    cup (0.198125)\n",
      "    espresso (0.005773)\n"
     ]
    }
   ],
   "source": [
    "imageFileName = 'CoffeeMug.jpg'\n",
    "image = testDs.getPreprocessedImage(imageFileName)\n",
    "classProbs = model.inferOne(image, returnProbs=True)\n",
    "top3Indexes = np.argsort(classProbs)[-3:][::-1]   # Indexes of classes with 3 highest probs (decreasing order)\n",
    "top3Porbs = classProbs[top3Indexes]\n",
    "print('Top-3 Classes (For \"%s\"):'%(imageFileName))\n",
    "for i in range(3):\n",
    "    print('    %s (%f)'%(ImageNetDSet.classNames[top3Indexes[i]], top3Porbs[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model\n",
    "This code runs inference on all images in the ImageNet dataset and compares the results with the ground truth labels in the testDs. The accuracy of the model is then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on 50000 Test Samples (batchSize:256, 4 towers) ... \n",
      "  Processed 50000 Sample. (Time: 45.06 Sec.)                              \n",
      "\n",
      "Observed Accuracy: 0.711160\n",
      "Top-5 Accuracy:   0.900680\n"
     ]
    }
   ],
   "source": [
    "myPrint('Running inference on %d Test Samples (batchSize:%d, %d towers) ... '%(testDs.numSamples,\n",
    "                                                                               testDs.batchSize,\n",
    "                                                                               len(model.towers)))\n",
    "results = model.evaluate(topK=5)    # Calculate and print top-5 accuracy as well as the default top-1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do I go from here?\n",
    "\n",
    "[Reducing number of parameters of MobileNetV2 Model](MobileNetV2-Reduce.ipynb)\n",
    "\n",
    "[Pruning MobileNetV2 Model](MobileNetV2-Prune.ipynb)\n",
    "\n",
    "[Quantizing MobileNetV2 Model](MobileNetV2-Quantize.ipynb)\n",
    "\n",
    "[Exporting MobileNetV2 Model to ONNX](MobileNetV2-ONNX.ipynb)\n",
    "\n",
    "[Exporting MobileNetV2 Model to CoreML](MobileNetV2-CoreML.ipynb)\n",
    "\n",
    "[Exporting MobileNetV2 Model to TensorFlow](MobileNetV2-TF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
