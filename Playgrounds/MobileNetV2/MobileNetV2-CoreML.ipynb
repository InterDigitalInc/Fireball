{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a MobileNetV2 model to CoreML\n",
    "To use a Fireball model in an iOS application, we can use ```exportToCoreMl``` method. This notebook shows how to use this function to create a CoreML model ready to be deployed in an iOS app. It assumes that a trained MobileNetV2 model already exists in the ```Models``` directory. Please refer to the notebook [Image Classification with MobileNetV2](MobileNetV2.ipynb) for more info about using a pretrained MobileNetV2 model.\n",
    "\n",
    "Fireball can also export models with reduced number of parameters, pruned models, and quatized models. Please refer to the following notebooks for more information:\n",
    "\n",
    "- [Reducing number of parameters of MobileNetV2 Model](MobileNetV2-Reduce.ipynb)\n",
    "- [Pruning MobileNetV2 Model](MobileNetV2-Prune.ipynb)\n",
    "- [Quantizing MobileNetV2 Model](MobileNetV2-Quantize.ipynb)\n",
    "\n",
    "Note: Fireball uses the ```coremltools``` python package to export CoreML models. If this is not installed you can just run the following command in a new cell and restart the kernel.\n",
    "```\n",
    "%pip install coremltools==3.4\n",
    "```\n",
    "\n",
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/MobileNetV2RRPRQR.fbm\" ... Done.\n",
      "Creating the fireball model \"MobileNetV2\" ... Done.\n",
      "\n",
      "Scope            InShape       Comments                 OutShape      Activ.   Post Act.        # of Params\n",
      "---------------  ------------  -----------------------  ------------  -------  ---------------  -----------\n",
      "IN_IMG                         Image Size: 224x224x3    224 224 3     None                      0          \n",
      "S1_L1_CONV       224 224 3     KSP: 3 2 0x1x0x1         112 112 32    None                      436        \n",
      "S1_L2_BN         112 112 32                             112 112 32    ReLU     x<6.0            128        \n",
      "S2_L1_DWCN       112 112 32    KSP: 3 1 s               112 112 32    None                      279        \n",
      "S2_L2_BN         112 112 32                             112 112 32    ReLU     x<6.0            128        \n",
      "S2_L3_CONV       112 112 32    KSP: 1 1 s               112 112 16    None                      265        \n",
      "S2_L4_BN         112 112 16                             112 112 16    None                      64         \n",
      "S3_L1_MN2        112 112 16    6 layers                 56 56 24      None                      4,636      \n",
      "S3_L2_MN1S       56 56 24      2 Paths, 6 layers        56 56 24      None                      7,537      \n",
      "S4_L1_MN2        56 56 24      6 layers                 28 28 32      None                      8,712      \n",
      "S4_L2_MN1S       28 28 32      2 Paths, 6 layers        28 28 32      None                      11,858     \n",
      "S4_L3_MN1S       28 28 32      2 Paths, 6 layers        28 28 32      None                      11,720     \n",
      "S5_L1_MN2        28 28 32      6 layers                 14 14 64      None                      17,196     \n",
      "S5_L2_MN1S       14 14 64      2 Paths, 6 layers        14 14 64      None                      36,913     \n",
      "S5_L3_MN1S       14 14 64      2 Paths, 6 layers        14 14 64      None                      36,636     \n",
      "S5_L4_MN1S       14 14 64      2 Paths, 6 layers        14 14 64      None                      36,765     \n",
      "S6_L1_MN1        14 14 64      6 layers                 14 14 96      None                      48,732     \n",
      "S6_L2_MN1S       14 14 96      2 Paths, 6 layers        14 14 96      None                      79,139     \n",
      "S6_L3_MN1S       14 14 96      2 Paths, 6 layers        14 14 96      None                      79,623     \n",
      "S7_L1_MN2        14 14 96      6 layers                 7 7 160       None                      111,339    \n",
      "S7_L2_MN1S       7 7 160       2 Paths, 6 layers        7 7 160       None                      201,110    \n",
      "S7_L3_MN1S       7 7 160       2 Paths, 6 layers        7 7 160       None                      202,341    \n",
      "S7_L4_MN1        7 7 160       6 layers                 7 7 320       None                      294,627    \n",
      "S8_L1_CONV       7 7 320       KSP: 1 1 s, LR184        7 7 1280      None                      164,595    \n",
      "S8_L2_BN         7 7 1280                               1 1 1280      ReLU     CLP->GAP         5,120      \n",
      "S8_L3_FC         1 1 1280      LR304                    1000          None                      453,817    \n",
      "OUT_CLASS        1000          1000 classes             1000          None                      0          \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "                                                                  Total Number of parameters: 1,813,716  \n",
      "  Processed 50000 Sample. (Time: 45.12 Sec.)                              \n",
      "\n",
      "Observed Accuracy: 0.652260\n"
     ]
    }
   ],
   "source": [
    "from fireball import Model\n",
    "from fireball.datasets.imagenet import ImageNetDSet\n",
    "gpus='0,1,2,3'\n",
    "\n",
    "# Create the test dataset for evaluation.\n",
    "testDs = ImageNetDSet.makeDatasets('Test', batchSize=256, preProcessing='Crop256Tf', numWorkers=8)\n",
    "\n",
    "# orgFileName = \"Models/MobileNetV2.fbm\"        # Original model\n",
    "# orgFileName = \"Models/MobileNetV2QR.fbm\"      # Quantized - Retrained\n",
    "# orgFileName = \"Models/MobileNetV2PR.fbm\"      # Pruned - Retrained\n",
    "# orgFileName = \"Models/MobileNetV2PRQR.fbm\"    # Pruned - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/MobileNetV2RR.fbm\"      # Reduced - Retrained\n",
    "# orgFileName = \"Models/MobileNetV2RRQR.fbm\"    # Reduced - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/MobileNetV2RRPR.fbm\"    # Reduced - Retrained - Pruned - Retrained\n",
    "orgFileName = \"Models/MobileNetV2RRPRQR.fbm\"  # Reduced - Retrained - Pruned - Retrained - Quantized - Retrained\n",
    "\n",
    "model = Model.makeFromFile(orgFileName, testDs=testDs, gpus=gpus)\n",
    "model.printLayersInfo()\n",
    "model.initSession()\n",
    "results = model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to CoreML\n",
    "CoreML handles the pre-processing of the images inside the model. The arguments ```rgbBias``` and ```scale``` are used to tell CoreML how to do this pre-processing. The pre-processed image is calculated by CoreML as:\n",
    "\n",
    "```\n",
    "processedImage = image * scale + rgbBias\n",
    "```\n",
    "\n",
    "For MobileNetV2, the processed images must be in RGB format with values normalized between -1 and 1. So, we are using scale=1/127.5 and rgbBias=-1.0, and setting isBgr to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting to CoreML model \"Models/MobileNetV2RRPRQR.mlmodel\" ... \n",
      "    Exported all 27 layers.                               \n",
      "    Saving to \"Models/MobileNetV2RRPRQR.mlmodel\" ... Done.\n",
      "Done (97.75 Sec.)\n",
      "Original Model File Size: 14,169,739 bytes\n",
      "CoreML Model File Size: 2,551,140 bytes (18.00% of original)\n"
     ]
    }
   ],
   "source": [
    "from fireball.datasets.imagenet import ImageNetDSet\n",
    "import os\n",
    "\n",
    "cmlFileName = orgFileName.replace('.fbm', '.mlmodel')\n",
    "\n",
    "model.exportToCoreMl(cmlFileName, classNames=ImageNetDSet.classNames,\n",
    "                     isBgr=False, scale=1./127.5, rgbBias=-1.0)\n",
    "\n",
    "orgFileSize = os.stat(\"Models/MobileNetV2.fbm\").st_size\n",
    "print('Original Model File Size: {:,} bytes'.format(orgFileSize))\n",
    "fileSize = os.stat(cmlFileName).st_size\n",
    "print('CoreML Model File Size: {:,} bytes ({:2.2%} of original)'.format(fileSize, fileSize/orgFileSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using netron to visualize the exported model\n",
    "We can now visualize the model's network structure using the \"netron\" package. If netron is not installed, you can just run the following command in a new cell and restart the kernel.\n",
    "\n",
    "```\n",
    "%pip install netron\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'Models/MobileNetV2RRPRQR.mlmodel' at http://10.21.16.50:8084\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':      # Running on MAC\n",
    "    netron.start(cmlFileName)   \n",
    "else:\n",
    "    import socket\n",
    "    hostIp = socket.gethostbyname(socket.gethostname())\n",
    "    netron.start(cmlFileName, address=(hostIp,8084))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the exported model\n",
    "To verify the exported model, we can now run inference on it. Currently the CoreML runtime is only available on Mac. You also need the pillow package because CoreML only accepts images in this format. If pillow is not installed, you can just run the following command in a new cell and restart the kernel.\n",
    "\n",
    "```\n",
    "%pip install pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-3 Classes (For \"CoffeeMug.jpg\"):\n",
      "    coffee_mug (0.918992)\n",
      "    cup (0.062491)\n",
      "    pitcher (0.011099)\n"
     ]
    }
   ],
   "source": [
    "assert platform.system() == 'Darwin', \"This is only supported when running on Mac!\"\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "imageFileName = 'CoffeeMug.jpg'\n",
    "\n",
    "img = cv2.imread(imageFileName)     # Reads image in BGR order\n",
    "img = np.float32(img)[..., ::-1]    # Convert to RGB\n",
    "\n",
    "imgSize = img.shape[:2]\n",
    "ratio = 256.0/min(imgSize)\n",
    "newSize = (int(np.round(imgSize[1]*ratio)), int(np.round(imgSize[0]*ratio)))\n",
    "\n",
    "# Note: INTER_AREA is best when shrinking and CV_INTER_CUBIC is best when enlarging\n",
    "img = cv2.resize(img, newSize,  interpolation = (cv2.INTER_AREA if ratio<1.0 else cv2.INTER_CUBIC))\n",
    "\n",
    "# Now crop the center 224x224 image\n",
    "dw = newSize[0] - 224\n",
    "dh = newSize[1] - 224\n",
    "resizedImg = img[dh//2:dh//2+224, dw//2:dw//2+224,:]\n",
    "pilImage = Image.fromarray( np.uint8(resizedImg) )\n",
    "\n",
    "\n",
    "import coremltools\n",
    "\n",
    "model = coremltools.models.MLModel(cmlFileName)\n",
    "outputDict = model.predict({'InputImage': pilImage}, useCPUOnly=True)\n",
    "\n",
    "labels, probs = zip(*outputDict['ClassProbs'].items())\n",
    "top3Idxs = np.argsort(probs)[-3:][::-1]    # Indexes of classes with 3 highest probs (decreasing order)\n",
    "print('Top-3 Classes (For \"%s\"):'%(imageFileName))\n",
    "for i in range(3):\n",
    "    print('    %s (%f)'%(labels[top3Idxs[i]], probs[top3Idxs[i]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do I go from here?\n",
    "\n",
    "[Exporting MobileNetV2 Model to ONNX](MobileNetV2-ONNX.ipynb)\n",
    "\n",
    "[Exporting MobileNetV2 Model to TensorFlow](MobileNetV2-TF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n",
    "[Image Classification with MobileNetV2](MobileNetV2.ipynb)\n",
    "\n",
    "[Reducing number of parameters of MobileNetV2 Model](MobileNetV2-Reduce.ipynb)\n",
    "\n",
    "[Pruning MobileNetV2 Model](MobileNetV2-Prune.ipynb)\n",
    "\n",
    "[Quantizing MobileNetV2 Model](MobileNetV2-Quantize.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
