{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a ResNet50 model to ONNX\n",
    "You can export any Fireball model to ONNX using the ```exportToOnnx``` function. This notebook shows how to use this function to create an ONNX model. It assumes that a trained ResNet50 model already exists in the ```Models``` directory. Please refer to the notebook [Image Classification with ResNet50](ResNet50.ipynb) for more info about using a pretrained ResNet50 model.\n",
    "\n",
    "Fireball can also export models with reduced number of parameters, pruned models, and quatized models. Please refer to the following notebooks for more information:\n",
    "\n",
    "- [Reducing number of parameters of ResNet50 Model](ResNet50-Reduce.ipynb)\n",
    "- [Pruning ResNet50 Model](ResNet50-Prune.ipynb)\n",
    "- [Quantizing ResNet50 Model](ResNet50-Quantize.ipynb)\n",
    "\n",
    "Note: Fireball uses the ```onnx``` python package to export models to ONNX. We also use the ```onnxruntime``` here to run and evaluate the onnx models. If these packages are not installed already, you can just run the following commands in a new cell and restart the kernel.\n",
    "```\n",
    "%pip install onnx==1.7.0\n",
    "%pip install onnxruntime==1.5.2\n",
    "```\n",
    "\n",
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/ResNet50RRPRQR.fbm\" ... Done.\n",
      "Creating the fireball model \"ResNet50\" ... Done.\n",
      "\n",
      "Scope            InShape       Comments                 OutShape      Activ.   Post Act.        # of Params\n",
      "---------------  ------------  -----------------------  ------------  -------  ---------------  -----------\n",
      "IN_IMG                         Image Size: 224x224x3    224 224 3     None                      0          \n",
      "S1_L1_CONV       224 224 3     KSP: 7 2 3               112 112 64    None                      7,119      \n",
      "S1_L2_BN         112 112 64                             56 56 64      ReLU     MP(KSP):3 2 1    256        \n",
      "S2_L1_RES2       56 56 64      2 Paths, 8 layers        56 56 256     ReLU                      48,621     \n",
      "S2_L2_RES1       56 56 256     2 Paths, 6 layers        56 56 256     ReLU                      48,346     \n",
      "S2_L3_RES1       56 56 256     2 Paths, 6 layers        56 56 256     ReLU                      48,863     \n",
      "S3_L1_RES2c1     56 56 256     2 Paths, 8 layers        28 28 512     ReLU                      177,495    \n",
      "S3_L2_RES1c2     28 28 512     2 Paths, 6 layers        28 28 512     ReLU                      137,995    \n",
      "S3_L3_RES1c3     28 28 512     2 Paths, 6 layers        28 28 512     ReLU                      82,708     \n",
      "S3_L4_RES1c4     28 28 512     2 Paths, 6 layers        28 28 512     ReLU                      151,144    \n",
      "S4_L1_RES2c5     28 28 512     2 Paths, 8 layers        14 14 1024    ReLU                      472,080    \n",
      "S4_L2_RES1c6     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      310,358    \n",
      "S4_L3_RES1c7     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      389,816    \n",
      "S4_L4_RES1c8     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      376,867    \n",
      "S4_L5_RES1c9     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      399,528    \n",
      "S4_L6_RES1c10    14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      407,956    \n",
      "S5_L1_RES2c11    14 14 1024    2 Paths, 8 layers        7 7 2048      ReLU     DO               1,635,702  \n",
      "S5_L2_RES1c12    7 7 2048      2 Paths, 6 layers        7 7 2048      ReLU     DO               1,109,156  \n",
      "S5_L3_RES1c13    7 7 2048      2 Paths, 6 layers        1 1 2048      ReLU     Global Avg       1,188,524  \n",
      "S6_L1_FC         1 1 2048      LR400                    1000          None                      913,776    \n",
      "OUT_CLASS        1000          1000 classes             1000          None                      0          \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "                                                                  Total Number of parameters: 7,906,310  \n",
      "  Processed 50000 Sample. (Time: 76.09 Sec.)                              \n",
      "\n",
      "Observed Accuracy: 0.682460\n"
     ]
    }
   ],
   "source": [
    "from fireball import Model\n",
    "from fireball.datasets.imagenet import ImageNetDSet\n",
    "gpus='0,1,2,3'\n",
    "\n",
    "# Create the test dataset for evaluation.\n",
    "testDs = ImageNetDSet.makeDatasets('Test', batchSize=256, preProcessing='Crop256Cafe', numWorkers=8)\n",
    "\n",
    "# orgFileName = \"Models/ResNet50.fbm\"        # Original model\n",
    "# orgFileName = \"Models/ResNet50QR.fbm\"      # Quantized - Retrained\n",
    "# orgFileName = \"Models/ResNet50PR.fbm\"      # Pruned - Retrained\n",
    "# orgFileName = \"Models/ResNet50PRQR.fbm\"    # Pruned - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/ResNet50RR.fbm\"      # Reduced - Retrained\n",
    "# orgFileName = \"Models/ResNet50RRQR.fbm\"    # Reduced - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/ResNet50RRPR.fbm\"    # Reduced - Retrained - Pruned - Retrained\n",
    "orgFileName = \"Models/ResNet50RRPRQR.fbm\"  # Reduced - Retrained - Pruned - Retrained - Quantized - Retrained\n",
    "\n",
    "model = Model.makeFromFile(orgFileName, testDs=testDs, gpus=gpus)\n",
    "model.printLayersInfo()\n",
    "model.initSession()\n",
    "results = model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model and check the exported ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting to ONNX model \"Models/ResNet50RRPRQR.onnx\" ... \n",
      "    Processed all 21 layers.                                      \n",
      "    Saving to \"Models/ResNet50RRPRQR.onnx\" ... Done.\n",
      "Done (55.66 Sec.)\n"
     ]
    }
   ],
   "source": [
    "onnxFileName = orgFileName.replace(\".fbm\",\".onnx\")\n",
    "\n",
    "model.exportToOnnx(onnxFileName, runQuantized=True, classNames=ImageNetDSet.classNames, \n",
    "                   modelDocStr=\"Fireball example: ResNet50 Model\")\n",
    "\n",
    "# Check the exported model. This throws exceptions if something is wrong with the exported model.\n",
    "import onnx\n",
    "from onnx import shape_inference\n",
    "\n",
    "onnxModel = onnx.load(onnxFileName)\n",
    "onnx.checker.check_model(onnxModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using netron to visualize the exported model\n",
    "We can now visualize the model's network structure using the \"netron\" package. If netron is not installed, you can just run the following command in a new cell and restart the kernel.\n",
    "\n",
    "```\n",
    "%pip install netron\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'Models/ResNet50RRPRQR.onnx' at http://10.21.16.50:8084\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':      # Running on MAC\n",
    "    netron.start(onnxFileName)   \n",
    "else:\n",
    "    import socket\n",
    "    hostIp = socket.gethostbyname(socket.gethostname())\n",
    "    netron.start(onnxFileName, address=(hostIp,8084))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the exported model\n",
    "To verify the exported model, we can now run inference on it. Here we load an image and do the required pre-processing before passing it to the exported model as input. We then print the top-3 most probable predicted labels for the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: InputImage, ouput: ClassProbs\n",
      "Top-3 Classes (For \"CoffeeMug.jpg\"):\n",
      "    coffee_mug (0.778762)\n",
      "    cup (0.121574)\n",
      "    pitcher (0.047829)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "imageFileName = 'CoffeeMug.jpg'\n",
    "\n",
    "img = cv2.imread(imageFileName)     # Reads image in BGR order\n",
    "\n",
    "# Resize the image to 256x256\n",
    "imgSize = img.shape[:2]\n",
    "ratio = 256.0/min(imgSize)\n",
    "newSize = (int(np.round(imgSize[1]*ratio)), int(np.round(imgSize[0]*ratio)))\n",
    "\n",
    "# Note: INTER_AREA is best when shrinking and CV_INTER_CUBIC is best when enlarging\n",
    "img = cv2.resize(img, newSize,  interpolation = (cv2.INTER_AREA if ratio<1.0 else cv2.INTER_CUBIC))\n",
    "\n",
    "# Now crop the center 224x224 image\n",
    "dw = newSize[0] - 224\n",
    "dh = newSize[1] - 224\n",
    "resizedImg = img[dh//2:dh//2+224, dw//2:dw//2+224,:]\n",
    "\n",
    "# Normalize the image using the mean values for blue, green, and red\n",
    "inputImage = (np.float32(resizedImg) - [103.939, 116.779, 123.68])\n",
    "inputImage = np.transpose(inputImage, (2,0,1))    # Onnx expects channel-first images\n",
    "\n",
    "# Inference using the ONNX model and \"onnxruntime\"\n",
    "import onnxruntime as ort\n",
    "session = ort.InferenceSession(onnxModel.SerializeToString(), None)\n",
    "    \n",
    "print('input: %s, ouput: %s'%(session.get_inputs()[0].name,session.get_outputs()[0].name))\n",
    "y = session.run(['ClassProbs'],{'InputImage':[inputImage]})\n",
    "\n",
    "classProbs = y[0][0]\n",
    "top3Indexes = np.argsort(classProbs)[-3:][::-1]    # Indexes of classes with 3 highest probs (decreasing order)\n",
    "top3Porbs = classProbs[top3Indexes]\n",
    "print('Top-3 Classes (For \"%s\"):'%(imageFileName))\n",
    "for i in range(3):\n",
    "    print('    %s (%f)'%(ImageNetDSet.classNames[top3Indexes[i]], top3Porbs[i])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do I go from here?\n",
    "\n",
    "[Exporting ResNet50 Model to CoreML](ResNet50-CoreML.ipynb)\n",
    "\n",
    "[Exporting ResNet50 Model to TensorFlow](ResNet50-TF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n",
    "[Image Classification with ResNet50](ResNet50.ipynb)\n",
    "\n",
    "[Reducing number of parameters of ResNet50 Model](ResNet50-Reduce.ipynb)\n",
    "\n",
    "[Pruning ResNet50 Model](ResNet50-Prune.ipynb)\n",
    "\n",
    "[Quantizing ResNet50 Model](ResNet50-Quantize.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
