{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a ResNet50 model to CoreML\n",
    "To use a Fireball model in an iOS application, we can use ```exportToCoreMl``` method. This notebook shows how to use this function to create a CoreML model ready to be deployed in an iOS app. It assumes that a trained ResNet50 model already exists in the ```Models``` directory. Please refer to the notebook [Image Classification with ResNet50](ResNet50.ipynb) for more info about using a pretrained ResNet50 model.\n",
    "\n",
    "Fireball can also export models with reduced number of parameters, pruned models, and quatized models. Please refer to the following notebooks for more information:\n",
    "\n",
    "- [Reducing number of parameters of ResNet50 Model](ResNet50-Reduce.ipynb)\n",
    "- [Pruning ResNet50 Model](ResNet50-Prune.ipynb)\n",
    "- [Quantizing ResNet50 Model](ResNet50-Quantize.ipynb)\n",
    "\n",
    "Note: Fireball uses the ```coremltools``` python package to export CoreML models. If this is not installed you can just run the following command in a new cell and restart the kernel.\n",
    "```\n",
    "%pip install coremltools==3.4\n",
    "```\n",
    "\n",
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/ResNet50RRPRQR.fbm\" ... Done.\n",
      "Creating the fireball model \"ResNet50\" ... Done.\n",
      "\n",
      "Scope            InShape       Comments                 OutShape      Activ.   Post Act.        # of Params\n",
      "---------------  ------------  -----------------------  ------------  -------  ---------------  -----------\n",
      "IN_IMG                         Image Size: 224x224x3    224 224 3     None                      0          \n",
      "S1_L1_CONV       224 224 3     KSP: 7 2 3               112 112 64    None                      7,120      \n",
      "S1_L2_BN         112 112 64                             56 56 64      ReLU     MP(KSP):3 2 1    256        \n",
      "S2_L1_RES2       56 56 64      2 Paths, 8 layers        56 56 256     ReLU                      48,614     \n",
      "S2_L2_RES1       56 56 256     2 Paths, 6 layers        56 56 256     ReLU                      48,346     \n",
      "S2_L3_RES1       56 56 256     2 Paths, 6 layers        56 56 256     ReLU                      48,873     \n",
      "S3_L1_RES2c1     56 56 256     2 Paths, 8 layers        28 28 512     ReLU                      177,500    \n",
      "S3_L2_RES1c2     28 28 512     2 Paths, 6 layers        28 28 512     ReLU                      137,989    \n",
      "S3_L3_RES1c3     28 28 512     2 Paths, 6 layers        28 28 512     ReLU                      82,706     \n",
      "S3_L4_RES1c4     28 28 512     2 Paths, 6 layers        28 28 512     ReLU                      151,147    \n",
      "S4_L1_RES2c5     28 28 512     2 Paths, 8 layers        14 14 1024    ReLU                      472,082    \n",
      "S4_L2_RES1c6     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      310,354    \n",
      "S4_L3_RES1c7     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      389,784    \n",
      "S4_L4_RES1c8     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      376,839    \n",
      "S4_L5_RES1c9     14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      399,527    \n",
      "S4_L6_RES1c10    14 14 1024    2 Paths, 6 layers        14 14 1024    ReLU                      407,968    \n",
      "S5_L1_RES2c11    14 14 1024    2 Paths, 8 layers        7 7 2048      ReLU     DO               1,635,749  \n",
      "S5_L2_RES1c12    7 7 2048      2 Paths, 6 layers        7 7 2048      ReLU     DO               1,109,162  \n",
      "S5_L3_RES1c13    7 7 2048      2 Paths, 6 layers        1 1 2048      ReLU     Global Avg       1,188,549  \n",
      "S6_L1_FC         1 1 2048      LR400                    1000          None                      913,769    \n",
      "OUT_CLASS        1000          1000 classes             1000          None                      0          \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "                                                                  Total Number of parameters: 7,906,334  \n",
      "  Processed 50000 Sample. (Time: 65.21 Sec.)                              \n",
      "\n",
      "Observed Accuracy: 0.683440\n"
     ]
    }
   ],
   "source": [
    "from fireball import Model\n",
    "from fireball.datasets.imagenet import ImageNetDSet\n",
    "gpus='0,1,2,3'\n",
    "\n",
    "# Create the test dataset for evaluation.\n",
    "testDs = ImageNetDSet.makeDatasets('Test', batchSize=256, preProcessing='Crop256Cafe', numWorkers=8)\n",
    "\n",
    "# orgFileName = \"Models/ResNet50.fbm\"        # Original model\n",
    "# orgFileName = \"Models/ResNet50QR.fbm\"      # Quantized - Retrained\n",
    "# orgFileName = \"Models/ResNet50PR.fbm\"      # Pruned - Retrained\n",
    "# orgFileName = \"Models/ResNet50PRQR.fbm\"    # Pruned - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/ResNet50RR.fbm\"      # Reduced - Retrained\n",
    "# orgFileName = \"Models/ResNet50RRQR.fbm\"    # Reduced - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/ResNet50RRPR.fbm\"    # Reduced - Retrained - Pruned - Retrained\n",
    "orgFileName = \"Models/ResNet50RRPRQR.fbm\"  # Reduced - Retrained - Pruned - Retrained - Quantized - Retrained\n",
    "\n",
    "model = Model.makeFromFile(orgFileName, testDs=testDs, gpus=gpus)   \n",
    "model.initSession()\n",
    "model.printLayersInfo()\n",
    "results = model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting to CoreML\n",
    "CoreML handles the pre-processing of the images inside the model. The arguments ```rgbBias``` and ```scale``` are used to tell CoreML how to do this pre-processing. The pre-processed image is calculated by CoreML as:\n",
    "\n",
    "```\n",
    "processedImage = image * scale + rgbBias\n",
    "```\n",
    "\n",
    "For ResNet50, the processed images must be in BGR format with values normalized using the mean values 103.939, 116.779, 123.68 for blue, green, and red respectively. So, we are using the following values:\n",
    "```\n",
    "scale=1        # This is the default\n",
    "rgbBias=[-123.68, -116.779, -103.939]\n",
    "isBgr=True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting to CoreML model \"Models/ResNet50RRPRQR.mlmodel\" ... \n",
      "    Exported all 21 layers.                               \n",
      "    Saving to \"Models/ResNet50RRPRQR.mlmodel\" ... Done.\n",
      "Done (256.33 Sec.)\n",
      "Original Model File Size: 102,562,787 bytes\n",
      "CoreML Model File Size: 10,149,763 bytes (9.90% of original)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cmlFileName = orgFileName.replace('.fbm', '.mlmodel')\n",
    "\n",
    "model.exportToCoreMl(cmlFileName, classNames=ImageNetDSet.classNames,\n",
    "                     isBgr=True, rgbBias=[-123.68, -116.779, -103.939])\n",
    "\n",
    "orgFileSize = os.stat(\"Models/ResNet50.fbm\").st_size\n",
    "print('Original Model File Size: {:,} bytes'.format(orgFileSize))\n",
    "fileSize = os.stat(cmlFileName).st_size\n",
    "print('CoreML Model File Size: {:,} bytes ({:2.2%} of original)'.format(fileSize, fileSize/orgFileSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using netron to visualize the exported model\n",
    "We can now visualize the model's network structure using the \"netron\" package. If netron is not installed, you can just run the following command in a new cell and restart the kernel.\n",
    "\n",
    "```\n",
    "%pip install netron\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'Models/ResNet50RRPRQR.mlmodel' at http://10.21.16.50:8084\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':      # Running on MAC\n",
    "    netron.start(cmlFileName)   \n",
    "else:\n",
    "    import socket\n",
    "    hostIp = socket.gethostbyname(socket.gethostname())\n",
    "    netron.start(cmlFileName, host=hostIp, port=8084)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the exported model\n",
    "To verify the exported model, we can now run inference on it. Currently the CoreML runtime is only available on Mac. You also need the pillow package because CoreML only accepts images in this format. If pillow is not installed, you can just run the following command in a new cell and restart the kernel.\n",
    "\n",
    "```\n",
    "%pip install pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "This is only supported when running on Mac!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dd78a9b80dc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Darwin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"This is only supported when running on Mac!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: This is only supported when running on Mac!"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "assert platform.system() == 'Darwin', \"This is only supported when running on Mac!\"\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "imageFileName = 'CoffeeMug.jpg'\n",
    "\n",
    "img = cv2.imread(imageFileName)     # Reads image in BGR order\n",
    "\n",
    "imgSize = img.shape[:2]\n",
    "ratio = 256.0/min(imgSize)\n",
    "newSize = (int(np.round(imgSize[1]*ratio)), int(np.round(imgSize[0]*ratio)))\n",
    "\n",
    "# Note: INTER_AREA is best when shrinking and CV_INTER_CUBIC is best when enlarging\n",
    "img = cv2.resize(img, newSize,  interpolation = (cv2.INTER_AREA if ratio<1.0 else cv2.INTER_CUBIC))\n",
    "\n",
    "# Now crop the center 224x224 image\n",
    "dw = newSize[0] - 224\n",
    "dh = newSize[1] - 224\n",
    "resizedImg = img[dh//2:dh//2+224, dw//2:dw//2+224,:]\n",
    "pilImage = Image.fromarray( np.uint8(resizedImg[:,:,::-1]) ) # Convert to PIL image (RGB)\n",
    "\n",
    "import coremltools\n",
    "\n",
    "model = coremltools.models.MLModel(cmlFileName)\n",
    "outputDict = model.predict({'InputImage': pilImage}, useCPUOnly=True)\n",
    "\n",
    "labels, probs = zip(*outputDict['ClassProbs'].items())\n",
    "top3Idxs = np.argsort(probs)[-3:][::-1]    # Indexes of classes with 3 highest probs (decreasing order)\n",
    "print('Top-3 Classes (For \"%s\"):'%(imageFileName))\n",
    "for i in range(3):\n",
    "    print('    %s (%f)'%(labels[top3Idxs[i]], probs[top3Idxs[i]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do I go from here?\n",
    "\n",
    "[Exporting ResNet50 Model to ONNX](ResNet50-ONNX.ipynb)\n",
    "\n",
    "[Exporting ResNet50 Model to TensorFlow](ResNet50-TF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n",
    "[Image Classification with ResNet50](ResNet50.ipynb)\n",
    "\n",
    "[Reducing number of parameters of ResNet50 Model](ResNet50-Reduce.ipynb)\n",
    "\n",
    "[Pruning ResNet50 Model](ResNet50-Prune.ipynb)\n",
    "\n",
    "[Quantizing ResNet50 Model](ResNet50-Quantize.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
