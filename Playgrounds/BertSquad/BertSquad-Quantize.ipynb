{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing the BERT/SQuAD Model\n",
    "This notebook shows how to quantize a pre-trained Fireball model using Codebook Quantization. It assumes \n",
    "that a trained ```BERT/SQuAD``` model already exists in the ```Models``` directory. Please refer to the notebook\n",
    "[Question Answering (BERT/SQuAD)](BertSquad.ipynb) for more info about training and using a BERT/SQuAD model.\n",
    "\n",
    "If you want to quantize a Low-Rank model, you can use [this](BertSquad-Reduce.ipynb) notebook\n",
    "to reduce the number of parameters in ```BERT/SQuAD```.\n",
    "\n",
    "Model quantization reduces the size of the model by using less number of bits for each floating \n",
    "point parameter. Fireball uses a codebook quantization method based on K-Means clustering algorithm.\n",
    "\n",
    "[quantizeModel](https://interdigitalinc.github.io/Fireball/html/source/model.html#fireball.model.Model.quantizeModel) is a class method that receives the file names of input and output to the \n",
    "quantization process. It also receives the quantization parameters such as ```minBits```, ```maxBits```, \n",
    "and ```mseUb```.\n",
    "\n",
    "Fireball can create models with 2-bit to 12-bit quantization (Codebook sizes 4 to 4096). For the quantized\n",
    "model to be compatible with [CoreML](https://developer.apple.com/documentation/coreml), we need to make sure the codebook size is a power of 2, less than or equal to 256, and only \"weight\" parameters are quantized (not biases)\n",
    "\n",
    "## Quantizing a pretrained model\n",
    "The code in the following cell quantizes the model specified by ```orgFileName``` and creates a new quantized model.\n",
    "\n",
    "For each parameter tensor of the model, we try quantization bits 2 to 8 and find the best quantization that satisfies the specified MSE value.\n",
    "\n",
    "To get better quantization (smaller model) increase ```mse```; to get better performance (larger model)\n",
    "use a smaller ```mse```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading model parameters from \"Models/BertSquadRRPR.fbm\" ... Done.\n",
      "Quantizing 271 tensors using 36 workers ... \n",
      "   Quantization Parameters:\n",
      "        mseUb .............. 1e-05\n",
      "        pdfFactor .......... 0.1\n",
      "        reuseEmptyClusters . True\n",
      "        weightsOnly ........ True\n",
      "        minBits ............ 2\n",
      "        maxBits ............ 8\n",
      "Quantization complete (61.05 Sec.).\n",
      "Now saving to \"Models/BertSquadRRPRQ.fbm\" ... Done.\n",
      "\n",
      "Size of Data: 223,350,332 -> 64,623,504 bytes\n",
      "Model File Size: 223,370,708 -> 64,648,420 bytes\n"
     ]
    }
   ],
   "source": [
    "from fireball import Model\n",
    "\n",
    "orgFileName = \"Models/BertSquadRRPR.fbm\"    # Reduced - Retrained - Pruned - Retrained\n",
    "quantizedFileName = orgFileName.replace('.fbm', 'Q.fbm')  # Append 'Q' to the filename for \"Quantized\"\n",
    "qResults = Model.quantizeModel(orgFileName, quantizedFileName,\n",
    "                               minBits=2, maxBits=8, mseUb=.00001, reuseEmptyClusters=True, \n",
    "                               weightsOnly=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the data size before and after quantization. \n",
    "\n",
    "## Evaluate the quantized model\n",
    "Let's see the impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer from \"/data/SQuAD/vocab.txt\" ... Done. (Vocab Size: 30522)\n",
      "\n",
      "Reading from \"Models/BertSquadRRPRQ.fbm\" ... Done.\n",
      "Creating the fireball model \"Bert-SQuAD\" ... Done.\n",
      "  Processed 10833 Samples. (Time: 77.99 Sec.)                              \n",
      "\n",
      "    Exact Match: 77.909\n",
      "    f1:          86.268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fireball.datasets.squad import SquadDSet\n",
    "gpus = \"upto4\"\n",
    "\n",
    "trainDs,testDs = SquadDSet.makeDatasets(\"Train,Test\", batchSize=128, version=1 )\n",
    "model = Model.makeFromFile(quantizedFileName, testDs=testDs, gpus=gpus)   \n",
    "model.initSession()\n",
    "results = model.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train and evaluate\n",
    "Fireball can retrain the quantized models by modifying the quantization codebooks. The following cell uses the training dataset to train the quantized model. It then evaluates the re-trained model and saves it to a new file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/BertSquadRRPRQ.fbm\" ... Done.\n",
      "Creating the fireball model \"Bert-SQuAD\" ... Done.\n",
      "\n",
      "Network configuration:\n",
      "  Input:                     A tuple of TokenIds and TokenTypes.\n",
      "  Output:                    2 logit vectors (with length â‰¤ 512) for start and end indexes of the answer.\n",
      "  Network Layers:            16\n",
      "  Tower Devices:             GPU0, GPU1, GPU2, GPU3\n",
      "  Total Network Parameters:  53,047,718\n",
      "  Total Parameter Tensors:   271\n",
      "  Trainable Tensors:         271\n",
      "  Training Samples:          87,844\n",
      "  Test Samples:              10,833\n",
      "  Num Epochs:                2\n",
      "  Batch Size:                32\n",
      "  L2 Reg. Factor:            0.0001\n",
      "  Global Drop Rate:          0   \n",
      "  Learning Rate:             0.000000005  \n",
      "  Optimizer:                 Adam\n",
      "\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "| Epoch  | Batch   | Learning Rate | Loss      | Valid/Test Acc.   |\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "| 1      | 2745    | 0.000000005   | 0.3401084 | N/A        77.91% |\n",
      "| 2      | 5491    | 0.000000005   | 0.3887257 | N/A        77.92% |\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "Total Training Time: 2914.97 Seconds\n",
      "  Processed 10833 Samples. (Time: 43.07 Sec.)                              \n",
      "\n",
      "    Exact Match: 77.919\n",
      "    f1:          86.272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model.makeFromFile(quantizedFileName, trainDs=trainDs, testDs=testDs,\n",
    "                           batchSize=32, numEpochs=2,\n",
    "                           learningRate=5e-9, optimizer='Adam',\n",
    "                           saveBest=False,\n",
    "                           gpus=gpus)\n",
    "model.printNetConfig()\n",
    "model.initSession()\n",
    "model.train()\n",
    "results = model.evaluate()\n",
    "\n",
    "retrainedFileName = quantizedFileName.replace('.fbm', 'R.fbm')  # Append 'R' to the filename for \"Re-trained\"\n",
    "model.save(retrainedFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the quantized model\n",
    "To reduce the model file size even more, you can use the [compressModel](https://interdigitalinc.github.io/Fireball/html/source/model.html#fireball.model.Model.compressModel) class method to compress the network parameters using arithmethic coding. This process is lossless and does not affect the model performance.\n",
    "\n",
    "Please note that while compressing a model makes it smaller, it takes longer to load a compressed model because each model parameter needs to go through the additional step of entropy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading model parameters from \"Models/BertSquadRRPRQR.fbm\" ... Done.\n",
      "Compressing 271 tensors using 36 workers ... \n",
      "Finished compressing model parameters (821.25 Sec.)\n",
      "Now saving to \"Models/BertSquadRRPRQR.fbmc\" ... Done.\n",
      "Model File Size: 64,648,394 -> 38,917,033 bytes\n"
     ]
    }
   ],
   "source": [
    "compressedFileName = retrainedFileName.replace('.fbm', '.fbmc')\n",
    "qResults = Model.compressModel(retrainedFileName, compressedFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also look at\n",
    "\n",
    "[Exporting BERT/SQuAD Model to ONNX](BertSquad-ONNX.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to TensorFlow](BertSquad-TF.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to CoreML](BertSquad-CoreML.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n",
    "[Question Answering (BERT/SQuAD)](BertSquad.ipynb)\n",
    "\n",
    "[Reducing number of parameters of BERT/SQuAD Model](BertSquad-Reduce.ipynb)\n",
    "\n",
    "[Pruning BERT/SQuAD Model](BertSquad-Prune.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
