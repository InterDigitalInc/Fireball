{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantizing the BERT/SQuAD Model\n",
    "This notebook shows how to quantize a pre-trained Fireball model using Codebook Quantization. It assumes \n",
    "that a trained ```BERT/SQuAD``` model already exists in the ```Models``` directory. Please refer to the notebook\n",
    "[Question Answering (BERT/SQuAD)](BertSquad.ipynb) for more info about training and using a BERT/SQuAD model.\n",
    "\n",
    "If you want to quantize a Low-Rank model, you can use [this](BertSquad-Reduce.ipynb) notebook\n",
    "to reduce the number of parameters in ```BERT/SQuAD```.\n",
    "\n",
    "Model quantization reduces the size of the model by using less number of bits for each floating \n",
    "point parameter. Fireball uses a codebook quantization method based on K-Means clustering algorithm.\n",
    "\n",
    "```quantizeModel``` is a class method that receives the file names of input and output to the \n",
    "quantization process. It also receives the quantization parameters such as ```minBits```, ```maxBits```, \n",
    "```mse```, and ```pdfFactor```.\n",
    "\n",
    "Fireball can create models with 2-bit to 12-bit quantization (Codebook sizes 4 to 4096). For the quantized\n",
    "model to be compatible with ```CoreML```, we need to make sure the codebook size is a power of 2, less than or equal to 256, and only \"weight\" parameters are quantized (not biases)\n",
    "\n",
    "## Quantizing a pretrained model\n",
    "The code in the following cell quantizes the model specified by ```orgFileName``` and creates a new quantized model.\n",
    "\n",
    "For each parameter tensor of the model, we try quantization bits 2 to 8 and find the best quantization that satisfies the specified MSE value.\n",
    "\n",
    "To get better quantization (smaller model) increase ```mse```; to get better performance (larger model)\n",
    "use a smaller ```mse```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading model parameters from \"Models/BertSquadRRPR.fbm\" ... Done.\n",
      "Quantizing 272 tensors using 76 workers ... \n",
      "   Quantization Parameters:\n",
      "        mseUb .............. 1e-05\n",
      "        pdfFactor .......... 0.1\n",
      "        reuseEmptyClusters . True\n",
      "        weightsOnly ........ True\n",
      "        minBits ............ 2\n",
      "        maxBits ............ 8\n",
      "Quantization complete (23.41 Sec.).\n",
      "Now saving to \"Models/BertSquadRRPRQ.fbm\" ... Done.\n",
      "\n",
      "Size of Data: 177,506,054 -> 53,384,271 bytes\n",
      "Model File Size: 177,526,445 -> 53,409,238 bytes\n"
     ]
    }
   ],
   "source": [
    "from fireball import Model\n",
    "\n",
    "# orgFileName = \"Models/SSD512.fbm\"        # Original model\n",
    "# orgFileName = \"Models/BertSquadP.fbm\"       # Pruned\n",
    "# orgFileName = \"Models/BertSquadPR.fbm\"      # Pruned - Retrained\n",
    "# orgFileName = \"Models/BertSquadR.fbm\"       # Reduced\n",
    "# orgFileName = \"Models/BertSquadRP.fbm\"      # Reduced - Pruned\n",
    "# orgFileName = \"Models/BertSquadRP.fbm\"      # Reduced - Pruned - Retrained\n",
    "# orgFileName = \"Models/BertSquadRR.fbm\"      # Reduced - Retrained\n",
    "# orgFileName = \"Models/BertSquadRRP.fbm\"     # Reduced - Retrained - Pruned\n",
    "orgFileName = \"Models/BertSquadRRPR.fbm\"    # Reduced - Retrained - Pruned - Retrained\n",
    "\n",
    "quantizedFileName = orgFileName.replace('.fbm', 'Q.fbm')  # Append 'Q' to the filename for \"Quantized\"\n",
    "qResults = Model.quantizeModel(orgFileName, quantizedFileName,\n",
    "                               minBits=2, maxBits=8, mseUb=.00001, reuseEmptyClusters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the data size before and after quantization. \n",
    "\n",
    "## Evaluate the quantized model\n",
    "Let's see the impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer from \"/data/SQuAD/vocab.txt\" ... Done. (Vocab Size: 30522)\n",
      "\n",
      "Reading from \"Models/BertSquadRRPRQ.fbm\" ... Done.\n",
      "Creating the fireball model \"Bert-SQuAD\" ... Done.\n",
      "  Processed 10833 Samples. (Time: 64.02 Sec.)                              \n",
      "\n",
      "    Exact Match: 77.692\n",
      "    f1:          85.868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fireball.datasets.squad import SquadDSet\n",
    "gpus = \"0,1,2,3\"\n",
    "\n",
    "trainDs,testDs = SquadDSet.makeDatasets(\"Train,Test\", batchSize=128, version=1 )\n",
    "model = Model.makeFromFile(quantizedFileName, testDs=testDs, gpus=gpus)   \n",
    "model.initSession()\n",
    "results = model.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train and evaluate\n",
    "Fireball can retrain the quantized models by modifying the quantization codebooks. The following cell uses the training dataset to train the quantized model.\n",
    "\n",
    "If the trained model specified by ```quantizedFileName``` is already available in the ```Models``` directory, this cell shows the results of last training. If you want to force it to do the training again, you can un-remark the line at the beginning of the cell to delete the existing file. Note that the re-training can take up to 2 hour on a 4-GPU machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/BertSquadRRPRQ.fbm\" ... Done.\n",
      "Creating the fireball model \"Bert-SQuAD\" ... Done.\n",
      "\n",
      "Network configuration:\n",
      "  Input:                     A tuple of TokenIds and TokenTypes.\n",
      "  Output:                    2 logit vectors (with length â‰¤ 512) for start and end indexes of the answer.\n",
      "  Network Layers:            16\n",
      "  Tower Devices:             GPU0, GPU1, GPU2, GPU3\n",
      "  Total Network Parameters:  41,818,535\n",
      "  Total Parameter Tensors:   272\n",
      "  Trainable Tensors:         272\n",
      "  Training Samples:          87,844\n",
      "  Test Samples:              10,833\n",
      "  Num Epochs:                2\n",
      "  Batch Size:                32\n",
      "  L2 Reg. Factor:            0.0001\n",
      "  Global Drop Rate:          0   \n",
      "  Learning Rate:             0.000000005  \n",
      "  Optimizer:                 Adam\n",
      "\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "| Epoch  | Batch   | Learning Rate | Loss      | Valid/Test Acc.   |\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "| 1      | 2746    | 0.000000005   | 0.434846  | N/A        77.72% |\n",
      "| 2      | 5492    | 0.000000005   | 0.4822826 | N/A        77.72% |\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "Total Training Time: 3820.65 Seconds\n",
      "  Processed 10833 Samples. (Time: 47.90 Sec.)                              \n",
      "\n",
      "    Exact Match: 77.720\n",
      "    f1:          85.881\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model.makeFromFile(quantizedFileName, trainDs=trainDs, testDs=testDs,\n",
    "                           batchSize=32, numEpochs=2,\n",
    "                           learningRate=5e-9, optimizer='Adam',\n",
    "                           saveBest=False,\n",
    "                           gpus=gpus)\n",
    "model.printNetConfig()\n",
    "model.initSession()\n",
    "model.train()\n",
    "results = model.evaluate()\n",
    "\n",
    "retrainedFileName = quantizedFileName.replace('.fbm', 'R.fbm')  # Append 'R' to the filename for \"Re-trained\"\n",
    "model.save(retrainedFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the quantized model\n",
    "To reduce the model file size even more, you can use the ``compressModel`` class method to compress the network parameters using arithmethic coding. This process is lossless and does not affect the model performance.\n",
    "\n",
    "Please note that while compressing a model makes it smaller, it takes longer to load a compressed model because each model parameter needs to go through the additional step of entropy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading model parameters from \"Models/BertSquadRRPRQR.fbm\" ... Done.\n",
      "Compressing 272 tensors using 76 workers ... \n",
      "Finished compressing model parameters (402.73 Sec.)\n",
      "Now saving to \"Models/BertSquadRRPRQR.fbmc\" ... Done.\n",
      "Model File Size: 53,409,212 -> 32,925,854 bytes\n"
     ]
    }
   ],
   "source": [
    "compressedFileName = retrainedFileName.replace('.fbm', '.fbmc')\n",
    "qResults = Model.compressModel(retrainedFileName, compressedFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do I go from here?\n",
    "\n",
    "[Exporting BERT/SQuAD Model to ONNX](BertSquad-ONNX.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to TensorFlow](BertSquad-TF.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to CoreML](BertSquad-CoreML.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n",
    "[Question Answering (BERT/SQuAD)](BertSquad.ipynb)\n",
    "\n",
    "[Reducing number of parameters of BERT/SQuAD Model](BertSquad-Reduce.ipynb)\n",
    "\n",
    "[Pruning BERT/SQuAD Model](BertSquad-Prune.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
