{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question answering with BERT and SQuAD\n",
    "Stanford Question Answering Dataset (SQuAD) (https://rajpurkar.github.io/SQuAD-explorer/) is a reading comprehension dataset, consisting of questions posed by crowd workers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
    "\n",
    "Bidirectional Encoder Representations from Transformers (BERT) (https://arxiv.org/abs/1810.04805) is a technique for NLP (Natural Language Processing) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google.\n",
    "\n",
    "In this playground we create a SQuAD model by using a pre-trained BERT base model as the backbone and adding an additional fully connected layer to the end of the model and train it using the SQuAD dataset.\n",
    "\n",
    "**Note:** If the SQuAD dataset is not available on this machine, the following code can take longer the first time it is executed as the dataset needs to be downloaded and intialized. Please be patient and avoid interrupting the process during the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from \"https://fireball.s3.us-west-1.amazonaws.com/data/SQuAD/SQuAD.zip\" ...\n",
      "Extracting \"/pa/home/hamidirads/data/SQuAD/SQuAD.zip\" ...\n",
      "Deleting \"/pa/home/hamidirads/data/SQuAD/SQuAD.zip\" ...\n",
      "Initializing tokenizer from \"/data/SQuAD/vocab.txt\" ... Done. (Vocab Size: 30522)\n",
      "SquadDSet Dataset Info:\n",
      "    Dataset Location ............................... /data/SQuAD/\n",
      "    Number of Training Samples ..................... 87844\n",
      "    Number of Test Samples ......................... 10833\n",
      "    Dataset Version ................................ 1\n",
      "    Max Seq. Len ................................... 384\n",
      "    +----------------------+--------------+--------------+\n",
      "    | Parameter            | Training     | Test         |\n",
      "    +----------------------+--------------+--------------+\n",
      "    | NumQuestions         | 87451        | 10570        |\n",
      "    | NumGoodQuestions     | 87451        | 10570        |\n",
      "    | NumAnswers           | 87844        | 35556        |\n",
      "    | NumContexts          | 18896        | 2067         |\n",
      "    | NumTitle             | 442          | 48           |\n",
      "    | MaxContextLen        | 853          | 789          |\n",
      "    | MaxQuestionLen       | 61           | 38           |\n",
      "    | NumImpossible        | 0            | 0            |\n",
      "    | MaxNumAnswers        | 1            | 6            |\n",
      "    | NumSegmented         | 893          | 183          |\n",
      "    | NumSamples           | 87844        | 10833        |\n",
      "    +----------------------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from fireball import Model, myPrint\n",
    "from fireball.datasets.squad import SquadDSet\n",
    "\n",
    "import time, os\n",
    "\n",
    "gpus = \"upto4\"\n",
    "\n",
    "# Preparing the dataset and model (Downloading them if necessary)\n",
    "SquadDSet.download()\n",
    "Model.downloadFromZoo(\"BertUncasedL12O768H12NoPool.npz\", \"./Models/\")\n",
    " \n",
    "trainDs,testDs = SquadDSet.makeDatasets(\"Train,Test\", batchSize=128, version=1 )\n",
    "SquadDSet.printDsInfo(trainDs, testDs)\n",
    "SquadDSet.printStats(trainDs, testDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Fireball model, print model information, and train on SQuAD dataset\n",
    "Now we create a model and initialize it's parameters from the BERT-base pre-trained model. The last fully connected layer is initialized randomly. The file ```BertUncasedL12O768H12NoPool.npz``` contains the parameters extracted from Google's original pre-trained BERT-base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/BertUncasedL12O768H12NoPool.npz\" ... Done.\n",
      "Creating the fireball model \"Bert-SQuAD\" ... \u001b[93m\n",
      "Warning: Only 197 of 199 parameter tensors initialized from the specified file!\u001b[0m\n",
      "Done.\n",
      "\n",
      "Scope            InShape       Comments                 OutShape      Activ.   Post Act.        # of Params\n",
      "---------------  ------------  -----------------------  ------------  -------  ---------------  -----------\n",
      "IN_EMB           ≤512 2                                 ≤512 768      None                      23,835,648 \n",
      "S1_L1_LN         ≤512 768                               ≤512 768      None     DO:0.1           1,536      \n",
      "S2_L1_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L2_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L3_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L4_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L5_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L6_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L7_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L8_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L9_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L10_BERT      ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L11_BERT      ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S2_L12_BERT      ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      7,087,872  \n",
      "S3_L1_FC         ≤512 768                               ≤512 2        None     L2               1,538      \n",
      "OUT_ANSWER       ≤512 2                                 2 ≤512        None                      0          \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "                                                                  Total Number of parameters: 108,893,186\n",
      "\n",
      "Network configuration:\n",
      "  Input:                     A tuple of TokenIds and TokenTypes.\n",
      "  Output:                    2 logit vectors (with length ≤ 512) for start and end indexes of the answer.\n",
      "  Network Layers:            16\n",
      "  Tower Devices:             GPU0, GPU1, GPU2, GPU3\n",
      "  Total Network Parameters:  108,893,186\n",
      "  Total Parameter Tensors:   199\n",
      "  Trainable Tensors:         199\n",
      "  Non-Transfered Tensors:    2\n",
      "  Training Samples:          87,844\n",
      "  Test Samples:              10,833\n",
      "  Num Epochs:                2\n",
      "  Batch Size:                32\n",
      "  L2 Reg. Factor:            0.0001\n",
      "  Global Drop Rate:          0   \n",
      "  Learning Rate: (Piecewise)\n",
      "    Pieces (Batch:value):    0:0.00004        1000:0.00003        4000:0.00002      \n",
      "    Train all parameters     After 100 batches\n",
      "  Optimizer:                 Adam\n",
      "\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "| Epoch  | Batch   | Learning Rate | Loss      | Valid/Test Acc.   |\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "| 1      | 2745    | 0.00003       | 1.4372726 | N/A        79.45% |\n",
      "| 2      | 5491    | 0.00002       | 0.8281896 | N/A        80.85% |\n",
      "+--------+---------+---------------+-----------+-------------------+\n",
      "Total Training Time: 3203.53 Seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "layersInfo='EMB_L512_O768_S.02:None,LN_E1e-12:None:DO_R0.1;' \\\n",
    "           '12*BERT_O768_I3072_H12:GELU;' \\\n",
    "           'FC_O2:None:L2R,ANSWER'\n",
    "\n",
    "# For the learning rate, we start at 0.00004 and train only the last fully connected\n",
    "# layer (fixing the main BERT models) for 100 batches. After that, we train the whole \n",
    "# model end-to-end for 900 more batches. Then we change the learning rate to 0.00003 \n",
    "# and train for 3000 batchs, before changing it to 0.00002 and train until end of training.\n",
    "learningRate = [(0,4e-5),(100,'trainAll'),(1000,3e-5),(4000,2e-5)]\n",
    "\n",
    "model = Model.makeFromFile(\"Models/BertUncasedL12O768H12NoPool.npz\",\n",
    "                           name='Bert-SQuAD', layersInfo=layersInfo,\n",
    "                           trainDs=trainDs, testDs=testDs,\n",
    "                           batchSize=32, numEpochs=2, regFactor=0.0001,\n",
    "                           learningRate=learningRate, optimizer='Adam',\n",
    "                           gpus=gpus)\n",
    "\n",
    "model.printLayersInfo()\n",
    "model.initSession()\n",
    "model.printNetConfig()\n",
    "model.train()\n",
    "model.save(\"Models/BertSquad.fbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Notes about the above results:\n",
    "- The warning message above means the 2 tensors (Weights and biases) for the fully connected layer ```S3_L1_FC``` were initialized randomly because they were not in the ```BertUncasedL12O768H12NoPool.npz``` model file. This is expected because we added this layer to use the multi-task BERT model for the task of Question Answering.\n",
    "- In the **Network Configuration** section, \"Non-Transfered Tensors\" means the ones that were not transfered from the pre-trained model and therefore initialized randomly as explained above. \n",
    "- The notation \"≤512\" in the \"InShape\" and \"OutShape\" columns means the sequence of tokens can be less than or equal to 512 for this model.\n",
    "- This is an example of how flexible the definition of \"Learning Rate\" is in Fireball. Review the comments and syntax of the learning rate in the above code.\n",
    "\n",
    "\n",
    "## Quick inference demonstration\n",
    "Here we have a \"context\" which is a paragraph about InterDigital copied from Wikipedia and 3 different questions related to the context. We use our model to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "InterDigital is a technology research and development company that provides wireless and video technologies for \n",
      "mobile devices, networks, and services worldwide. Founded in 1972, InterDigital is listed on NASDAQ and is \n",
      "included in the S&P SmallCap 600. InterDigital had 2020 revenue of $359 million and a portfolio of about \n",
      "32,000 U.S. and foreign issued patents and patent applications.\n",
      "\n",
      "\n",
      "Q1: When was InterDigital established?\n",
      "    1972\n",
      "\n",
      "Q2: How much was InterDigital's revenue in 2020?\n",
      "    $359 million \n",
      "\n",
      "Q3: What does InterDigital provide?\n",
      "    wireless and video technologies \n"
     ]
    }
   ],
   "source": [
    "context = r\"\"\"\n",
    "InterDigital is a technology research and development company that provides wireless and video technologies for \n",
    "mobile devices, networks, and services worldwide. Founded in 1972, InterDigital is listed on NASDAQ and is \n",
    "included in the S&P SmallCap 600. InterDigital had 2020 revenue of $359 million and a portfolio of about \n",
    "32,000 U.S. and foreign issued patents and patent applications.\n",
    "\"\"\"\n",
    "\n",
    "print(context)\n",
    "questions = [\n",
    "    \"When was InterDigital established?\",\n",
    "    \"How much was InterDigital's revenue in 2020?\",\n",
    "    \"What does InterDigital provide?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    sample, spans = testDs.tokenizer.makeModelInput(context, question, returnSpans=True)\n",
    "\n",
    "    startTok, endTok = model.inferOne(sample)\n",
    "\n",
    "    answer = testDs.tokenizer.getTextFromTokSpan(sample[0], context, spans[0], startTok, endTok)\n",
    "    print(\"\\nQ%d: %s\\n    %s\"%(i+1, question, answer))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "This code runs inference on all questions in the test dataset and compares the results with the ground-truth. The evaluation scores are calculated based on the original evaluation code provided by SQuAD dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/BertSquad.fbm\" ... Done.\n",
      "Creating the fireball model \"Bert-SQuAD\" ... Done.\n",
      "  Processed 10833 Samples. (Time: 55.83 Sec.)                              \n",
      "\n",
      "    Exact Match: 80.851\n",
      "    f1:          87.902\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model.makeFromFile(\"Models/BertSquad.fbm\", testDs=testDs, gpus=gpus)   \n",
    "model.initSession()\n",
    "results = model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also look at\n",
    "\n",
    "[Reducing number of parameters of BERT/SQuAD Model](BertSquad-Reduce.ipynb)\n",
    "\n",
    "[Pruning BERT/SQuAD Model](BertSquad-Prune.ipynb)\n",
    "\n",
    "[Quantizing BERT/SQuAD Model](BertSquad-Quantize.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to ONNX](BertSquad-ONNX.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to TensorFlow](BertSquad-TF.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to CoreML](BertSquad-CoreML.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
