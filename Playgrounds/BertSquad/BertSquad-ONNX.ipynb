{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting a BERT/SQuAD model to ONNX\n",
    "You can export any Fireball model to ONNX using the ```exportToOnnx``` function. This notebook shows how to use this function to create an ONNX model. It assumes that a trained BERT/SQuAD model already exists in the ```Models``` directory. Please refer to the notebook [Question Answering (BERT/SQuAD)](BertSquad.ipynb) for more info about training and using a BERT/SQuAD model.\n",
    "\n",
    "Fireball can also export models with reduced number of parameters, pruned models, and quatized models. Please refer to the following notebooks for more information:\n",
    "\n",
    "- [Reducing number of parameters of BERT/SQuAD Model](BertSquad-Reduce.ipynb)\n",
    "- [Pruning BERT/SQuAD Model](BertSquad-Quantize.ipynb)\n",
    "- [Quantizing BERT/SQuAD Model](BertSquad-Quantize.ipynb)\n",
    "\n",
    "\n",
    "Note: Fireball uses the ```onnx``` python package to export models to ONNX. We also use the ```onnxruntime``` here to run and evaluate the onnx models. If these packages are not installed already, you can just run the following commands in a new cell and restart the kernel.\n",
    "```\n",
    "%pip install onnx==1.7.0\n",
    "%pip install onnxruntime==1.5.2\n",
    "```\n",
    "\n",
    "## Load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading from \"Models/BertSquadRRPRQR.fbm\" ... Done.\n",
      "Creating the fireball model \"Bert-SQuAD\" ... Done.\n",
      "\n",
      "Scope            InShape       Comments                 OutShape      Activ.   Post Act.        # of Params\n",
      "---------------  ------------  -----------------------  ------------  -------  ---------------  -----------\n",
      "IN_EMB           ≤512 2        LR512                    ≤512 768      None                      4,743,508  \n",
      "S1_L1_LN         ≤512 768                               ≤512 768      None     DO:0.1           1,536      \n",
      "S2_L1_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      2,839,343  \n",
      "S2_L2_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      2,898,736  \n",
      "S2_L3_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      2,909,600  \n",
      "S2_L4_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,037,894  \n",
      "S2_L5_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,126,202  \n",
      "S2_L6_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,136,500  \n",
      "S2_L7_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,095,255  \n",
      "S2_L8_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,102,207  \n",
      "S2_L9_BERT       ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,161,666  \n",
      "S2_L10_BERT      ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,303,363  \n",
      "S2_L11_BERT      ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,264,215  \n",
      "S2_L12_BERT      ≤512 768      768/3072, 12 heads       ≤512 768      GELU                      3,197,595  \n",
      "S3_L1_FC         ≤512 768                               ≤512 2        None     L2               915        \n",
      "OUT_ANSWER       ≤512 2                                 2 ≤512        None                      0          \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "                                                                  Total Number of parameters: 41,818,535 \n"
     ]
    }
   ],
   "source": [
    "from fireball import Model\n",
    "\n",
    "# orgFileName = \"Models/BertSquad.fbm\"        # Original model\n",
    "# orgFileName = \"Models/BertSquadQR.fbm\"      # Quantized - Retrained\n",
    "# orgFileName = \"Models/BertSquadPR.fbm\"      # Pruned - Retrained\n",
    "# orgFileName = \"Models/BertSquadPRQR.fbm\"    # Pruned - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/BertSquadRR.fbm\"      # Reduced - Retrained\n",
    "# orgFileName = \"Models/BertSquadRRQR.fbm\"    # Reduced - Retrained - Quantized - Retrained\n",
    "# orgFileName = \"Models/BertSquadRRPR.fbm\"    # Reduced - Retrained - Pruned - Retrained\n",
    "orgFileName = \"Models/BertSquadRRPRQR.fbm\"  # Reduced - Retrained - Pruned - Retrained - Quantized - Retrained\n",
    "\n",
    "model = Model.makeFromFile(orgFileName, gpus='0')\n",
    "model.printLayersInfo()\n",
    "model.initSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model and check the exported ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting to ONNX model \"Models/BertSquadRRPRQR.onnx\" ... \n",
      "    Processed all 16 layers.                                     \n",
      "    Saving to \"Models/BertSquadRRPRQR.onnx\" ... Done.\n",
      "Done (44.44 Sec.)\n"
     ]
    }
   ],
   "source": [
    "onnxFileName = orgFileName.replace(\".fbm\",\".onnx\")\n",
    "\n",
    "doc = (\"This is the question answering model based on BERTbase and fine-tuned on SQuAD dataset. \" + \n",
    "       \"The inputs are a list of token IDs and a list of token types based on word-piece vocabulary embedding \" +\n",
    "       \"scheme. The token IDs list must start with a [CLS] and end with an [SEP] code. The question tokens and \" +\n",
    "       \"context tokens must also be separated by another [SEP] code. If the inputs are fed to the model in \" +\n",
    "       \"batches of more than one, they must be padded with the [PAD] code so that they all have the same \" +\n",
    "       \"length. The token types input must have 0's for question tokens and 1's for context tokens.\")\n",
    "model.exportToOnnx(onnxFileName, runQuantized=True, modelDocStr=doc)\n",
    "\n",
    "# Check the exported model. This throws exceptions if something is wrong with the exported model.\n",
    "import onnx\n",
    "from onnx import shape_inference\n",
    "\n",
    "onnxModel = onnx.load(onnxFileName)\n",
    "onnx.checker.check_model(onnxModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using netron to visualize the exported model\n",
    "We can now visualize the model's network structure using the \"netron\" package. If netron is not installed, you can just run the following command in a new cell and restart the kernel.\n",
    "\n",
    "```\n",
    "%pip install netron\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'Models/BertSquadRRPRQR.onnx' at http://10.1.16.58:8084\n"
     ]
    }
   ],
   "source": [
    "import netron\n",
    "import platform\n",
    "\n",
    "if platform.system() == 'Darwin':      # Running on MAC\n",
    "    netron.start(onnxFileName)   \n",
    "else:\n",
    "    import socket\n",
    "    hostIp = socket.gethostbyname(socket.gethostname())\n",
    "    netron.start(onnxFileName, address=(hostIp,8084))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference on the exported model\n",
    "To verify the exported model, we can now run inference on it. Here we have a \"context\" which is a paragraph about InterDigital copied from Wikipedia and 3 different questions related to the context. We use our exported ONNX model to answer the questions.\n",
    "\n",
    "**Note:** We could use the \"Tokenizer\" included in Fireball. But to show the independence of the following code from Fireball, we are using Google's original tokenizer from [here](https://github.com/google-research/bert/blob/master/tokenization.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "InterDigital is a mobile technology research and development company that provides wireless and video technologies\n",
      "for mobile devices, networks, and services worldwide. Founded in 1972, InterDigital is listed on NASDAQ and is\n",
      "included in the S&P MidCap 400 index. InterDigital had 2018 revenue of $280 million and a portfolio of about\n",
      "34,000 U.S. and foreign issued patents and patent applications.\n",
      "\n",
      "\n",
      "Q1: When was InterDigital established?\n",
      "    1972\n",
      "\n",
      "Q2: How much was InterDigital's revenue in 2018?\n",
      "    $ 280 million\n",
      "\n",
      "Q3: What does InterDigital provide?\n",
      "    wireless and video technologies for mobile devices , networks , and services worldwide\n"
     ]
    }
   ],
   "source": [
    "context = r\"\"\"\n",
    "InterDigital is a mobile technology research and development company that provides wireless and video technologies\n",
    "for mobile devices, networks, and services worldwide. Founded in 1972, InterDigital is listed on NASDAQ and is\n",
    "included in the S&P MidCap 400 index. InterDigital had 2018 revenue of $280 million and a portfolio of about\n",
    "34,000 U.S. and foreign issued patents and patent applications.\n",
    "\"\"\"\n",
    "\n",
    "print(context)\n",
    "questions = [\n",
    "    \"When was InterDigital established?\",\n",
    "    \"How much was InterDigital's revenue in 2018?\",\n",
    "    \"What does InterDigital provide?\",\n",
    "]\n",
    "\n",
    "import tokenization\n",
    "tokenizer = tokenization.FullTokenizer(\"/data/SQuAD/vocab.txt\")\n",
    "\n",
    "import onnx\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "session = ort.InferenceSession(onnxModel.SerializeToString(), None)\n",
    "\n",
    "contextTokens = tokenizer.tokenize(context)\n",
    "for i, question in enumerate(questions):\n",
    "    questionTokens = tokenizer.tokenize(question)\n",
    "    allTokens = [\"[CLS]\"] + questionTokens + [\"[SEP]\"] + contextTokens + [\"[SEP]\"]\n",
    "    tokIds = tokenizer.convert_tokens_to_ids(allTokens)\n",
    "    tokTypes = [0]*(len(questionTokens)+2) + [1]*(len(contextTokens)+1)\n",
    "    \n",
    "    startLogits, endLogits = session.run(['StartLogits','EndLogits'],{'TokIds':[tokIds], 'TokTypes':[tokTypes]})\n",
    "    startTok, endTok = np.argmax(startLogits), np.argmax(endLogits)\n",
    "    startTok -= len(questionTokens) + 2\n",
    "    endTok -= len(questionTokens) + 2\n",
    "    answer = ' '.join(contextTokens[int(startTok):int(endTok+1)])\n",
    "    print(\"\\nQ%d: %s\\n    %s\"%(i+1, question, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where do I go from here?\n",
    "\n",
    "[Exporting BERT/SQuAD Model to CoreML](BertSquad-CoreML.ipynb)\n",
    "\n",
    "[Exporting BERT/SQuAD Model to TensorFlow](BertSquad-TF.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "[Fireball Playgrounds](../Contents.ipynb)\n",
    "\n",
    "[Question Answering (BERT/SQuAD)](BertSquad.ipynb)\n",
    "\n",
    "[Reducing number of parameters of BERT/SQuAD Model](BertSquad-Reduce.ipynb)\n",
    "\n",
    "[Pruning BERT/SQuAD Model](BertSquad-Prune.ipynb)\n",
    "\n",
    "[Quantizing BERT/SQuAD Model](BertSquad-Quantize.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
